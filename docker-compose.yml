version: '3'
services:

  qdrant:
    image: qdrant/qdrant
    ports:
      - "6333:6333"
    volumes:
      - qdrant_storage:/qdrant/storage

  python-app:
    container_name: python-app
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - ./:/app
    working_dir: /app
    environment:
      - PYTHONPATH=/app
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
    depends_on:
      - qdrant
      - postgres
    command: ["tail", "-f", "/dev/null"]
    # command: "uvicorn backend:app --reload --host 0.0.0.0 --port 8000"
    ports:
      - '8000:8000'

  postgres:
    image: postgres:14.1-alpine
    container_name: postgres
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: postgresql
    ports:
      - '5432:5432'
    volumes:
      - /data/postgresql_data:/var/lib/postgresql/data
  
  openWebUI:
    container_name: openwebui
    image: ghcr.io/open-webui/open-webui:main
    restart: always
    ports:
      - "3000:8080"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - open-webui-local:/app/backend/data

  ollama:
    container_name: ollama
    build:
      context: ./components/LLM
      dockerfile: Dockerfile
    ports:
      - "11434:11434"
    volumes:
      - ollama-local:/root/.ollama
    
  tei:
    image: ghcr.io/huggingface/text-embeddings-inference:cuda-sha-cb1e594
    
    ports:
      - '80:80'
    container_name: tei_container
    hostname: tei
    restart: on-failure
    command:
      - "--model-id"
      - "sentence-transformers/paraphrase-MiniLM-L6-v2"
    volumes:
      - /data
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]

volumes:
  qdrant_storage:
  ollama-local:
    # external: true
  open-webui-local:
    # external: true